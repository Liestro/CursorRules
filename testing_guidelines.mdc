---
description: Rules for writing effective tests across the project
globs: **/*test*.*
---

## General Principles
1. Every new feature must have corresponding tests before being merged.
2. Tests should be independent and isolated - one test should not depend on another test's execution.
3. Follow the AAA pattern: Arrange (setup), Act (execute), Assert (verify).
4. Test both expected behavior and edge cases, including error handling.
5. Keep tests simple, focused, and readable - each test should verify one specific behavior.

## Naming Conventions
6. Test files should be named with the pattern `{module_name}_test.{ext}` or `test_{module_name}.{ext}` depending on language conventions.
7. Test functions/methods should clearly describe what they're testing using the pattern `test_should_{expected_behavior}_when_{condition}`.
8. Use descriptive variable names in tests that clearly indicate their purpose, even if longer than usual.

## Test Structure
9. Group related tests into test classes or describe blocks according to language conventions.
10. Use setup and teardown methods/hooks for common test initialization and cleanup.
11. Separate unit tests, integration tests, and end-to-end tests into different directories or files.
12. Include appropriate comments explaining complex test scenarios or non-obvious assertions.

## Mocking and Test Data
13. Use mocks, stubs, or fakes for external dependencies to ensure tests remain isolated.
14. Create reusable test fixtures for common test data to maintain consistency.
15. Avoid using production data in tests; create synthetic test data that covers all test scenarios.
16. Clearly identify mock objects in variable names (e.g., `mock_database`, `stub_api_client`).

## Assertions
17. Use specific assertion methods rather than generic equality checks when available.
18. Include meaningful error messages in assertions to aid debugging when tests fail.
19. Verify only what's relevant to the current test; avoid asserting unrelated state.

## Examples

```python
# Example for Python using pytest
def test_should_calculate_correct_total_when_given_valid_items():
    # Arrange
    items = [
        {"name": "Product 1", "price": 10.0, "tax_rate": 0.1},
        {"name": "Product 2", "price": 20.0, "tax_rate": 0.2}
    ]
    
    # Act
    result = calculate_total(items)
    
    # Assert
    expected = 10.0 * 1.1 + 20.0 * 1.2
    assert result == pytest.approx(expected), f"Expected {expected}, got {result}"
```

```javascript
// Example for JavaScript using Jest
describe('User validation', () => {
  test('should throw error when input is empty', () => {
    // Arrange
    const emptyInput = '';
    
    // Act & Assert
    expect(() => {
      validateUserInput(emptyInput);
    }).toThrow('Input must be a non-empty string');
  });
  
  test('should return trimmed lowercase string when input is valid', () => {
    // Arrange
    const input = '  Test String  ';
    
    // Act
    const result = validateUserInput(input);
    
    // Assert
    expect(result).toBe('test string');
  });
});
```

## Test Coverage
20. Aim for at least 80% code coverage for critical business logic.
21. Focus on testing behavior and outcomes rather than implementation details.
22. Include both positive tests (expected behavior) and negative tests (error handling).
23. Test boundary conditions and edge cases explicitly.

## Performance
24. Keep unit tests fast - they should execute in milliseconds.
25. Separate slow integration or end-to-end tests from the main test suite.
26. Use test parameterization to test multiple input variations without code duplication.

## Maintenance
27. Refactor tests when refactoring code to maintain alignment.
28. Delete tests that no longer serve a purpose rather than commenting them out.
29. Review test failures promptly - a failing test indicates either a bug or an outdated test.
30. Update tests when requirements change to ensure they remain relevant. 